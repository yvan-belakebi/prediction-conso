{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prévision de la consommation d'électricité en France.\n",
    "\n",
    "### Abstract\n",
    "Chaque jour, le gestionnaire de réseau RTE (Réseau de Transport de l'Electricité) ajuste la production de manière à fournir à chacun, à chaque instant, la quantité d'électricité nécessaire à sa consommation. Cet ajustement doit être particulièrement précis car les lois de la physiques imposent que la quantité d'électricité qui entre dans le système doit être égale à la quantité qui en sort.\n",
    "\n",
    "D'autre part, du fait de la nature des facteurs de production, qui peuvent être intermittent ou au contraire ajustables (ex: il est possible d'allumer et d'éteindre une centrale à charbon sur demande), il est important de connaître à l'avance la demande en électricité de manière à choisir à l'avance les facteur de production à utiliser. \n",
    "\n",
    "Dans ce contexte il apparaît que prédire les consommation de l'énergie est un enjeu majeur. \n",
    "\n",
    "Toutefois, les outils de la statistiques permettent de construire des modèles capables de prédire la consommation de manière suffisamment précise pour éviter tant les courts-circuits que les pénuries. En effet, malgré l'incertitude, la consommation d'électricité semble être fortement influencée par différents facteur. \n",
    "\n",
    "Quels sont alors ces facteurs ? Comment les déterminer ? Comment batir un modèle de prédiction de la consommation sufisamment efficace ?\n",
    "\n",
    "Afin de déterminer ceux-ci, nous allons faire appelle dans une première partie à l'étude des statistiques descriptives des bases de données fournies par RTE sur la consommation et la production. Dans un second temps, nous utiliserons nous construirons un modèle de régression linéaire afin de prédire la demande \n",
    "\n",
    "\n",
    "Pour réaliser notre projet, principalement les données publiques fournies par RTE \n",
    "Plan : \n",
    "\n",
    "0.[Acquisition et nettoyage des données](#Acquisition)\n",
    "\n",
    "1.[Statistiques descriptives](#Statistiques)\n",
    "\n",
    "2.[Modèle de Régression](#Modèle)\n",
    "\n",
    "3.[Conclusion](#Conclusion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached geopandas-0.8.1-py2.py3-none-any.whl (962 kB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.0.0.post1-cp38-cp38-win_amd64.whl (14.4 MB)\n",
      "Requirement already satisfied: pandas>=0.23.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from geopandas) (1.1.3)\n",
      "Collecting shapely\n",
      "  Using cached Shapely-1.7.1-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting fiona\n",
      "  Using cached Fiona-1.8.18.tar.gz (1.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     command: 'C:\\Users\\nicol\\anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\nicol\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9z4p3ita\\\\fiona\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\nicol\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9z4p3ita\\\\fiona\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\nicol\\AppData\\Local\\Temp\\pip-pip-egg-info-qm_k4ewa'\n",
      "         cwd: C:\\Users\\nicol\\AppData\\Local\\Temp\\pip-install-9z4p3ita\\fiona\\\n",
      "    Complete output (1 lines):\n",
      "    A GDAL API version must be specified. Provide a path to gdal-config using a GDAL_CONFIG environment variable or use a GDAL_VERSION environment variable.\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0)Acquisition et nettoyage des données \n",
    "\n",
    "Météo-France met à disposition des données météorologiques remontant jusqu'à 1996 et avec une fréquence d'enregistrement de 3h. Ces données sont téléchargeables en tables mensuelles à l'url https://public.opendatasoft.com/explore/dataset/donnees-synop-essentielles-omm/information/?refine.date=2020%2F12%2F09&fbclid=IwAR0RXR54Jve66dUu5yQqMc6hEwnGjedb6BcPQGI3Rmk0DcfdPIGCWN8fLNI .En concaténant les tables mensuelles pour les années 2018, 2019 et 2020 et en ne gardant que les variables que l'on souhaite étudier, on obtient une base de donnée synthétique, avec de quelques impuretés. Il est donc nécessaire de la nettoyer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7005</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>280.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7015</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>281.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7020</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>283.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7027</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>280.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7037</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>279.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    ID            date       t\n",
       "0           0  7005  20180101000000  280.15\n",
       "1           1  7015  20180101000000  281.05\n",
       "2           2  7020  20180101000000  283.45\n",
       "3           3  7027  20180101000000  280.55\n",
       "4           4  7037  20180101000000  279.55"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import nan\n",
    "Gbase=pd.read_csv(\"https://raw.githubusercontent.com/yvan-belakebi/prediction-conso/contrib-yvan-bis/gbase\")\n",
    "basemod=Gbase.copy()\n",
    "basemod=basemod.replace(\"mq\",nan)\n",
    "basemod=basemod.replace(\"t\", nan)\n",
    "basemod=basemod.dropna()\n",
    "basemod[\"t\"]=basemod[\"t\"].astype(float)\n",
    "basemod[\"date\"]=basemod[\"date\"].astype(str)\n",
    "basemod[\"numer_sta\"]=basemod[\"numer_sta\"].astype(int)\n",
    "basemod=basemod.rename(columns={'numer_sta':'ID'})\n",
    "basemod.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc la température mesurée par chaque station, à toutes les dates. L'objectif est de transformer cette température en heating degree days et cooling degree days. Heating degree days est la différence si elle est positive entre une température de référence, à partir de laquelle on considère que les ménages allument le chauffage, et la température mesurée. Cooling degree days est son pendant pour la climatisation. On va ensuite passer ces variables à l'échelle nationale en effectuant une moyenne pondérée par la population représentée par chaque station. Pour cela, on va attribuer à chaque station le département dans lequel elle se situe par reverse geocoding, et estimer la population représentée par une station à la population du département sur le nombre de stations dans le département."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_departement</th>\n",
       "      <th>Département</th>\n",
       "      <th>2017</th>\n",
       "      <th>test</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>Nord</td>\n",
       "      <td>2604361</td>\n",
       "      <td>True</td>\n",
       "      <td>0.039149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>Paris</td>\n",
       "      <td>2187526</td>\n",
       "      <td>True</td>\n",
       "      <td>0.032883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rhône</td>\n",
       "      <td>2024162</td>\n",
       "      <td>True</td>\n",
       "      <td>0.030427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>Circonscription départementale du Rhône</td>\n",
       "      <td>1843319</td>\n",
       "      <td>True</td>\n",
       "      <td>0.027709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>Seine-Saint-Denis</td>\n",
       "      <td>1623111</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code_departement                              Département    2017   test  \\\n",
       "59               59                                     Nord  2604361  True   \n",
       "75               75                                    Paris  2187526  True   \n",
       "13               13                         Bouches-du-Rhône  2024162  True   \n",
       "69               69  Circonscription départementale du Rhône  1843319  True   \n",
       "93               93                        Seine-Saint-Denis  1623111  True   \n",
       "\n",
       "      weight  \n",
       "59  0.039149  \n",
       "75  0.032883  \n",
       "13  0.030427  \n",
       "69  0.027709  \n",
       "93  0.024399  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "url_popdep=\"https://fr.wikipedia.org/wiki/Liste_des_d%C3%A9partements_fran%C3%A7ais_class%C3%A9s_par_population_et_superficie\"\n",
    "\n",
    "request_text=request.urlopen(url_popdep).read()\n",
    "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "tableau_dep=page.find('table',{\"class\":\"wikitable sortable alternance\"})\n",
    "table_body=tableau_dep.find('tbody')\n",
    "rows=table_body.find_all('tr')\n",
    "\n",
    "dico_dep = dict()\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols[1:]]\n",
    "    if len(cols) > 0 :\n",
    "        dico_dep[cols[0]] = cols[0:]\n",
    "\n",
    "data_dep = pd.DataFrame.from_dict(dico_dep,orient='index')\n",
    "#data_dep.to_csv(\"C:/Users/ADMIN/Documents/projet python 2A/data_dep\")\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('th')\n",
    "    if len(cols) > 0 :\n",
    "        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n",
    "        columns_dep = cols\n",
    "\n",
    "import re\n",
    "\n",
    "columns_années = [re.sub('\\[ (\\d+) \\] ?', '', nom_col) for nom_col in columns_dep]\n",
    "columns_dep=['Code','Département']+columns_années+['superficie','densité']\n",
    "\n",
    "data_dep.columns = columns_dep[0:]\n",
    "popdep=data_dep[['Code','Département','2017 ']]\n",
    "\n",
    "\n",
    "def transfo(s):\n",
    "    return re.sub(\"\\D\",\"\",s)\n",
    "\n",
    "station_dep=pd.read_csv(\"https://raw.githubusercontent.com/yvan-belakebi/prediction-conso/main/station_finale\")\n",
    "station_dep=pd.merge(basemod,station_dep)\n",
    "popdep=popdep.rename(columns={'Code':'code_departement'})\n",
    "popdep=popdep.astype({'2017 ':'str'}, errors='ignore')\n",
    "popdep['2017 ']=popdep['2017 '].apply(transfo)\n",
    "popdep['test']=popdep['2017 '].str.isdigit()\n",
    "popdep=popdep.query('test')\n",
    "\n",
    "popdep=popdep.astype({'2017 ':'int'})\n",
    "\n",
    "popdep['weight']=popdep['2017 ']/popdep['2017 '].sum()\n",
    "popdep.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>t</th>\n",
       "      <th>code_departement</th>\n",
       "      <th>Département</th>\n",
       "      <th>2017</th>\n",
       "      <th>test</th>\n",
       "      <th>weight</th>\n",
       "      <th>hdd</th>\n",
       "      <th>cdd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7005</td>\n",
       "      <td>20180101000000</td>\n",
       "      <td>280.15</td>\n",
       "      <td>80</td>\n",
       "      <td>Somme</td>\n",
       "      <td>572443</td>\n",
       "      <td>True</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160686</td>\n",
       "      <td>7299</td>\n",
       "      <td>20181226150000</td>\n",
       "      <td>273.45</td>\n",
       "      <td>68</td>\n",
       "      <td>Haut-Rhin</td>\n",
       "      <td>764030</td>\n",
       "      <td>True</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279090</td>\n",
       "      <td>7627</td>\n",
       "      <td>20190902150000</td>\n",
       "      <td>293.85</td>\n",
       "      <td>9</td>\n",
       "      <td>Ariège</td>\n",
       "      <td>153153</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    ID            date       t  code_departement Département  \\\n",
       "0           0  7005  20180101000000  280.15                80       Somme   \n",
       "1      160686  7299  20181226150000  273.45                68   Haut-Rhin   \n",
       "2      279090  7627  20190902150000  293.85                 9      Ariège   \n",
       "\n",
       "    2017   test    weight   hdd  cdd  \n",
       "0  572443  True  0.008605   9.0  0.0  \n",
       "1  764030  True  0.011485  15.7  0.0  \n",
       "2  153153  True  0.002302   0.0  0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_pop=pd.merge(station_dep,popdep, on='code_departement')\n",
    "station_pop['code_departement']=station_pop['code_departement'].replace(\"2A\",\"2\")\n",
    "station_pop['code_departement']=station_pop['code_departement'].replace(\"2B\",\"2\")\n",
    "station_pop=station_pop.astype({\"code_departement\":'int'})\n",
    "\n",
    "\n",
    "tbasse=289.15  #16°C, température à partir de laquelle on chauffe\n",
    "thaute=301.15  #28°C, température à partir de laquelle on climatise\n",
    "\n",
    "station_pop['hdd']=np.maximum(0, tbasse-station_pop['t'])\n",
    "station_pop['cdd']=np.maximum(0, station_pop['t']-thaute)\n",
    "\n",
    "station_pop.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit ensuite de réunir les bases station_pop et consommation avec un merge pour obtenir une base prête pour la régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Statistiques descriptives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on s'intéresse à la base de donnée sur la consommation d'électricité à l'échelle de la France durant l'année 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_fr_2018 = pd.read_csv('https://raw.githubusercontent.com/yvan-belakebi/prediction-conso/main/cons2018.csv',encoding='ISO-8859-1',error_bad_lines=False,sep = ';')\n",
    "# On supprime les lignes avec des valeurs manquantes\n",
    "cons_fr_2018 = cons_fr_2018.drop(['Nature'],axis=1)\n",
    "cons_fr_2018.dropna(inplace=True)\n",
    "# On restreint la base de données aux variables qui nous intéresse.\n",
    "cons_fr_2018 = cons_fr_2018.iloc[:,0:13]\n",
    "cons_fr_2018.head(10)\n",
    "# On crée une variable correspondant aux mois.\n",
    "cons_fr_2018['Date'] = pd.to_datetime(cons_fr_2018['Date'],format=\"%d/%m/%Y\")\n",
    "cons_fr_2018['Mois'] = cons_fr_2018['Date'].astype(str).str[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va s'intéresser au profil de production des énergies suivantes à la fois au niveau mensuel et au niveau journalier pour les énergies renouvelables. En effent, il n'est pas très intéressant de comparer les renouvelables avec les autres sources de production comme les centrales à charbon car on peut allumer et éteindre une centrale à charbon tandis que les renouvelables produisent tout le temps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne_j_eol = cons_fr_2018.groupby('Heures')['Eolien'].mean().plot()\n",
    "moyenne_j_sol = cons_fr_2018.groupby('Heures')['Solaire'].mean().plot()\n",
    "moyenne_j_gaz = cons_fr_2018.groupby('Heures')['Gaz'].mean().plot()\n",
    "plt.title('Profil de production journalier des différentes énergies')\n",
    "plt.ylabel('Production en MW')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne_a_eol = cons_fr_2018.groupby('Mois')['Eolien'].mean().plot()\n",
    "moyenne_a_sol = cons_fr_2018.groupby('Mois')['Solaire'].mean().plot()\n",
    "moyenne_a_gaz = cons_fr_2018.groupby('Mois')['Gaz'].mean().plot()\n",
    "plt.title('Profil de production annuel des différentes énergies')\n",
    "plt.ylabel('Production en MW')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On voit bien ici que les différentes sources de production ont des profil différents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on veut comprendre plus en détail le profil journalier moyen de la consommation. Nous cherchons donc à faire un graphique permettant de d'avoir une idée des variation intrajournalières de la consommation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne_cons = cons_fr_2018.groupby('Heures')['Consommation'].mean()\n",
    "plt.title('Variations intrajournalières de la consommation en France en 2018')\n",
    "plt.ylabel('Consommation en MW')\n",
    "moyenne_cons.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même idée mais pour la consommation annuelles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne_cons = cons_fr_2018.groupby('Mois')['Consommation'].mean()\n",
    "plt.title('Variations intramensuelles de la consommation en France en 2018')\n",
    "plt.ylabel('Consommation en MW')\n",
    "moyenne_cons.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la variable mois \n",
    "# On passe la date au format date \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la lumière des deux graphique que nous venons de construire, il apparaît qu'il y a deux effet qui jouent sur la consommation d'électricité en France. On distingue donc un effet journalier : les gens ont tendance à consommer beaucoup plus d'électricité le jour que la nuit. Plus précisément, la courbe est creuse entre 00h00 et 05h00 tandis qu'elle est à son pic entre 10h00 et 20h00. \n",
    "\n",
    "Le second effet est mensuel. La consommation est plus élevée en hiver qu'en été. Notre intuition là-dessus est que les gens chauffe beaucoup avec des chauffage électrique lorsqu'il fait froid tandis qu'en été, les gens ont moins tendance à utiliser le climatiseur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Modèle de régression linéaire \n",
    "\n",
    "Afin de constuire un modèle de régression linéaire permettant de prédire au mieux la consommation, nous sommes partie de notre intuition. En effet, en plus de leur et de la période de l'année, la température semble pourrait jouer une rôle important sur la demande. Ainsi on crée deux variables qui renvoient aux températures suffisamment haute pour déclencher l'utilisation du climatiseur et aux variables suffisamment basses pour déclencher l'utilisation du radiateur. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = pd.read_csv('https://raw.githubusercontent.com/yvan-belakebi/prediction-conso/contrib-yvan-bis/base_synth%C3%A9')\n",
    "# Création de la variable 'Heure'\n",
    "reg_table['Heures'] = reg_table['date'].astype(str).str[8:10].astype(str)\n",
    "# On crée les dummy variable qui nous intéressent\n",
    "reg_table['HB_s1'] = reg_table['Heures'] == '03'\n",
    "reg_table['HB_s2'] = reg_table['Heures'] == '00'\n",
    "reg_table['HB_s3'] = reg_table['Heures'] == '06'\n",
    "reg_table['HB'] = reg_table['HB_s1'].astype(int) + reg_table['HB_s2'].astype(int) + reg_table['HB_s3'].astype(int)\n",
    "\n",
    "reg_table['HH_s1'] = reg_table['Heures'] == '09'\n",
    "reg_table['HH_s2'] = reg_table['Heures'] == '12'\n",
    "reg_table['HH_s3'] = reg_table['Heures'] == '15'\n",
    "reg_table['HH_s4'] = reg_table['Heures'] == '18'\n",
    "reg_table['HH_s5'] = reg_table['Heures'] == '21'\n",
    "reg_table['HH'] = reg_table['HH_s1'].astype(int) + reg_table['HH_s2'].astype(int)+ reg_table['HH_s3'].astype(int) + reg_table['HH_s4'].astype(int)+ reg_table['HH_s5']\n",
    "# On entraîne notre modèle\n",
    "x_train, x_test, y_train, y_test = train_test_split(reg_table[['cdd_pond','hdd_pond','HH','HB']],\n",
    "                                                    reg_table[['Consommation']],\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(x_train, y_train)\n",
    "ols.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols.intercept_, ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.OLS(reg_table[\"Consommation\"].astype(float), reg_table[['cdd_pond','hdd_pond','HH','HB']].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sm = ols_model.fit()\n",
    "results_sm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test - ols.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, notre modèle de régression linéaire peut donc s'écrire : \n",
    "\n",
    "$ Consommation = (-208)\\times cdd_{pond} + 2244\\times hdd_{pond} + 45770\\times HH + 40640\\times HB + \\epsilon $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Interprétation économétrique du modèle\n",
    "Si on prend on compte les résultats de statmodel, on remarque qu'il n'ya pas d'intercept. Toutefois ils permettent d'entrevoir certaines tendance. Comme l'avions supposé, la variable cdd_pond qui correspond à la variation de consommation causée par des température élevée est la moins pertinente du modèle. En effet, on ne peut pas rejeter l'hypothèse que cdd_pond soit nul. C'est l'intuition que nous avions eu en regardant le graphique de la variation de la consommation au cours d'une année. \n",
    " \n",
    "On remarque que les variables HH et HB sont les plus significatives du modèle. Elle correspondent à ce que l'on pourrait appeler des heures pleines et des heures creuses. Si on prend en compte la régression donnée par sklearn, la variablle corrélée très positivement à la consommation tandis que la variable HB ( heures creuses ) est corrélée négativement. \n",
    "\n",
    "Le Scatter plot montre que les résidus de notre régression sont plus élevées lorsque les valeurs observées sont plus élevées. Cela contredit l'hypothèse d'homoscedascticité. \n",
    "\n",
    "Enfin la valeur du R² est de 0.626 ce qui n'est pas suffisant comme prédiction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Conclusion\n",
    "\n",
    "Notre régression explique une partie de la consommation mais le R² est trop faible pour que les préctions soient viables. \n",
    "Dans le but d'améliorer notre modèle, nous voyons les pistes d'amélioration suivante : \n",
    "\n",
    "- Ajouter des variables calendaires permettant de faire la distinction entre les jours fériés et les jours ouvrés. Il nous semble que la demande d'électricité varie également beaucoup selon si on est en semaine ou le week-end. \n",
    "\n",
    "- Optimiser les températures seuil de manière à avoir le meilleur R². \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
